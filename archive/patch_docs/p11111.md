좋아, 바로 꽂는다. 핵심만 깔끔히 패치해 줄게—동일시각 처리, 두바이 타임존, 월 피벗/누적 흐름을 “정석”대로 고정. (각 호출은 pandas 공식 API에 맞춰 썼고, melt/pivot_table/to_datetime/tz_localize·tz_convert/cumsum 레퍼런스는 아래에 붙였어.)

# 패치 1) `flow_ledger_v2.py` — 타임라인→전이 레저 고정

아래 블록을 **덮어쓰기**(또는 동일 위치 교체)하면 돼. 요지가:

* **tz-naive → tz_localize('Asia/Dubai')**, **tz-aware → tz_convert('Asia/Dubai')** 후 월 버킷 생성. ([Pandas][1])
* 같은 케이스·같은 시각·같은 창고는 **합산(sum)**, 서로 다른 창고가 같은 시각이면 **연쇄 전이(OUT→IN)**로 기록.
* 월 피벗은 `pivot_table(..., aggfunc='sum', fill_value=0)` 표준 형태. ([Pandas][2])
* 누적은 `(입고−출고).cumsum()`으로 계산. ([Pandas][3])

```python
# --- 타임존/월 버킷: 두바이 기준 ---
DUBAI_TZ = "Asia/Dubai"

def _to_dubai_aware(ts: pd.Series) -> pd.Series:
    s = pd.to_datetime(ts, errors="coerce")  # NaT 보존(혼합 tz 파싱 경고 대응) :contentReference[oaicite:3]{index=3}
    if getattr(s.dt, "tz", None) is None:
        s = s.dt.tz_localize(DUBAI_TZ)       # naive → 로컬라이즈(시간 이동 없음) :contentReference[oaicite:4]{index=4}
    else:
        s = s.dt.tz_convert(DUBAI_TZ)        # aware → 두바이로 변환        :contentReference[oaicite:5]{index=5}
    return s

def _to_ym_dubai(ts: pd.Series) -> pd.Series:
    return _to_dubai_aware(ts).dt.strftime("%Y-%m")

# --- 동일시각 규칙(전이 보존 + 같은 창고 합산) ---
STAGE_PRIO = {"pre_arrival": 0, "shipping": 1, "site": 2, "warehouse": 3}  # 창고가 최종 승자
WH_PRIO = { "DSV Al Markaz":10,"DSV Indoor":20,"DSV Outdoor":30,"AAA Storage":40,
            "Hauler Indoor":50,"DSV MZP":60,"JDN MZD":70,"MOSB":80,"DHL WH":90 }

def build_flow_ledger(master_df: pd.DataFrame,
                      case_cols=("Case No","Case","Case_ID","Case_Number","case_no","case"),
                      qty_cols=("Pkg","pkg","Pkg_Quantity","pkg_quantity","quantity","qty")):
    ...
    # 1) 창고/사이트 날짜열만 long으로 변환 (melt 정석) :contentReference[oaicite:6]{index=6}
    loc_cols = [c for c in df.columns if _canon(c, amap) in (WAREHOUSES | SITES)]
    long = df[[col_case, col_qty] + loc_cols].melt(
        id_vars=[col_case, col_qty], var_name="loc", value_name="ts"
    ).dropna(subset=["ts"]).copy()

    # 2) 두바이 기준 시간/월버킷
    long["ts"] = _to_dubai_aware(long["ts"])
    long["Year_Month"] = long["ts"].dt.strftime("%Y-%m")

    # 3) 정규화/정렬 키
    long["loc"]        = long["loc"].map(lambda s: _canon(s, amap))
    long["stage"]      = long["loc"].map(_stage_of)
    long["stage_prio"] = long["stage"].map(STAGE_PRIO)
    long["wh_prio"]    = long["loc"].map(lambda s: WH_PRIO.get(s, 999))
    long["qty"]        = long[col_qty].astype("float").fillna(0).astype(int)
    long               = long[long["qty"] > 0]

    # 4) 같은 시각·같은 창고 합산 → 같은 시각 내 다창고는 전이로 해석
    same_wh = (long
        .groupby([col_case, "ts", "loc", "stage", "stage_prio", "wh_prio"], as_index=False)["qty"]
        .sum()
        .sort_values([col_case, "ts", "stage_prio", "wh_prio"])
    )

    events, edges = [], []

    # (a) 동일시각 내 '다창고' 체인 전이: A→B→C ... → OUT/IN 생성
    for case, gts in same_wh.groupby(col_case, sort=False):
        for ts, one_ts in gts.groupby("ts", sort=False):
            one_ts   = one_ts.sort_values(["stage_prio","wh_prio"])
            wh_list  = [r.loc for r in one_ts.itertuples() if r.loc in WAREHOUSES]
            if len(wh_list) >= 2:
                for prev, curr in zip(wh_list[:-1], wh_list[1:]):
                    qty = int(one_ts.loc[one_ts["loc"] == prev, "qty"].iloc[0])
                    ym  = ts.strftime("%Y-%m")
                    edges.append((str(case), ts, prev, curr, qty))
                    events.append(Event(str(case), ym, "OUT", prev, qty, ts, src=prev, dst=curr))
                    events.append(Event(str(case), ym, "IN",  curr, qty, ts, src=prev, dst=curr))

    # (b) 연속 시점 간 전이: 상태가 바뀔 때만 IN/OUT
    for case, g in same_wh.groupby(col_case, sort=False):
        prev_loc = None
        for r in g.itertuples(index=False):
            loc, ts, qty = r.loc, r.ts, int(r.qty)
            ym = ts.strftime("%Y-%m")
            if prev_loc is None:
                if loc in WAREHOUSES:
                    events.append(Event(str(case), ym, "IN", loc, qty, ts, src=None, dst=loc))
            else:
                if prev_loc != loc:
                    if prev_loc in WAREHOUSES and loc in WAREHOUSES:
                        edges.append((str(case), ts, prev_loc, loc, qty))
                        events.append(Event(str(case), ym, "OUT", prev_loc, qty, ts, src=prev_loc, dst=loc))
                        events.append(Event(str(case), ym, "IN",  loc,      qty, ts, src=prev_loc, dst=loc))
                    elif prev_loc in WAREHOUSES and loc not in WAREHOUSES:
                        events.append(Event(str(case), ym, "OUT", prev_loc, qty, ts, src=prev_loc, dst=loc))
                    elif prev_loc not in WAREHOUSES and loc in WAREHOUSES:
                        events.append(Event(str(case), ym, "IN",  loc,      qty, ts, src=prev_loc, dst=loc))
            prev_loc = loc

    # 5) 월×창고×입/출 집계(피벗 fill_value=0) :contentReference[oaicite:7]{index=7}
    if not events:
        cols = ["Year_Month","Warehouse","Kind","Qty"]
        return pd.DataFrame(columns=cols), pd.DataFrame(columns=["case","ts","src","dst","qty"])
    ev = pd.DataFrame([e.__dict__ for e in events])
    ledger = (ev.groupby(["ym", "warehouse", "kind"], as_index=False)["qty"].sum()
                .rename(columns={"ym":"Year_Month","warehouse":"Warehouse","kind":"Kind","qty":"Qty"}))
    edges_df = pd.DataFrame(edges, columns=["case","ts","src","dst","qty"]).sort_values(["case","ts"])
    return ledger, edges_df

def monthly_inout_table(ledger: pd.DataFrame, warehouses: Optional[List[str]] = None) -> pd.DataFrame:
    if ledger is None or ledger.empty:
        return pd.DataFrame(columns=["입고월"])
    warehouses = warehouses or _warehouse_labels()
    piv = (ledger.pivot_table(index="Year_Month", columns=["Warehouse","Kind"], values="Qty",
                              aggfunc="sum", fill_value=0, sort=True))  # 피벗 표준 시그니처 :contentReference[oaicite:8]{index=8}
    out = pd.DataFrame({"입고월": piv.index})
    for w in warehouses:
        ins  = piv.get((w, "IN"),  pd.Series(0, index=piv.index))
        outs = piv.get((w, "OUT"), pd.Series(0, index=piv.index))
        out[f"입고_{w}"] = ins.astype(int).values
        out[f"출고_{w}"] = outs.astype(int).values
        out[f"누적_{w}"] = (ins - outs).cumsum().astype(int).values  # 표준 누적합 :contentReference[oaicite:9]{index=9}
    return out.reset_index(drop=True)
```

# 패치 2) `report_generator.py` — 호출부 교체

월별 시트를 만들던 함수에서 **아래 4줄 흐름**만 보장해. (melt/피벗/누적은 위 모듈이 다 처리)

```python
from flow_ledger_v2 import build_flow_ledger, monthly_inout_table, sanity_report, _warehouse_labels

master_df = stats.get("master") or stats.get("case_master") or getattr(self, "combined_data", None) or pd.DataFrame()
ledger_df, edges_df = build_flow_ledger(master_df)
df_monthly = monthly_inout_table(ledger_df, _warehouse_labels())

bad = sanity_report(df_monthly)
if bad:
    raise ValueError(f"[FlowLedger mismatch] {bad[:6]}")
```

# 빠른 자가검증(필수)

```python
w = "DSV Indoor"
assert df_monthly[f"입고_{w}"].sum() - df_monthly[f"출고_{w}"].sum() == int(df_monthly[f"누적_{w}"].iloc[-1])
```

---

# 왜 이번엔 맞나(짧게 근거)

* **melt → long** 변환은 pandas가 권장하는 와이드→롱 패턴. ([Pandas][4])
* **pivot_table(fill_value=0)** 으로 빈 조합 0 채움(공식 인자). ([Pandas][2])
* **tz_localize/tz_convert**로 로컬 시간대 정합성을 보장한 뒤 월 버킷 생성. (naive→localize, aware→convert) ([Pandas][1])
* **cumsum**으로 월 정렬 후 누적합을 계산하는 게 표준. ([Pandas][3])

---

이 상태로 돌리고, `DSV Indoor`의 최종 검산(∑입고−∑출고=마지막 누적) 캡처만 보여줘. 거기서 남는 오차가 있으면 **해당 월·케이스** 로그로 바로 파고들게.

[1]: https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.tz_localize.html?utm_source=chatgpt.com "pandas.Series.dt.tz_localize — pandas 2.3.3 documentation"
[2]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html?utm_source=chatgpt.com "pandas.pivot_table — pandas 2.3.3 documentation - PyData |"
[3]: https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html?utm_source=chatgpt.com "pandas.Series.cumsum — pandas 2.3.3 documentation"
[4]: https://pandas.pydata.org/docs/user_guide/reshaping.html?utm_source=chatgpt.com "Reshaping and pivot tables — pandas 2.3.1 documentation"
